{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2_61ALUMiPGz"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "\u003e http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIpuAlVyicwB"
      },
      "source": [
        "## Problem Setup\n",
        "\n",
        "This is a simple example of recall-constrained optimization on simulated data: we seek a classifier that minimizes the average hinge loss while constraining recall to be at least 90%.\n",
        "\n",
        "We'll start with the required imports\u0026mdash;notice the definition of `tfco`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0PXL-Tkcip04"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use the GitHub version of TFCO\n",
        "!pip install git+https://github.com/google-research/tensorflow_constrained_optimization\n",
        "import tensorflow_constrained_optimization as tfco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-wj8ptvZiuCz"
      },
      "source": [
        "We'll next create a simple simulated dataset by sampling 1000 random 10-dimensional feature vectors from a Gaussian, finding their labels using a random \\\"ground truth\\\" linear model, and then adding noise by randomly flipping 200 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mT3kJE4sixf6"
      },
      "outputs": [],
      "source": [
        "# Create a simulated 10-dimensional training dataset consisting of 1000 labeled\n",
        "# examples, of which 800 are labeled correctly and 200 are mislabeled.\n",
        "num_examples = 1000\n",
        "num_mislabeled_examples = 200\n",
        "dimension = 10\n",
        "# We will constrain the recall to be at least 90%.\n",
        "recall_lower_bound = 0.9\n",
        "\n",
        "# Create random \"ground truth\" parameters for a linear model.\n",
        "ground_truth_weights = np.random.normal(size=dimension) / math.sqrt(dimension)\n",
        "ground_truth_threshold = 0\n",
        "\n",
        "# Generate a random set of features for each example.\n",
        "features = np.random.normal(size=(num_examples, dimension)).astype(\n",
        "    np.float32) / math.sqrt(dimension)\n",
        "# Compute the labels from these features given the ground truth linear model.\n",
        "labels = (np.matmul(features, ground_truth_weights) \u003e\n",
        "          ground_truth_threshold).astype(np.float32)\n",
        "# Add noise by randomly flipping num_mislabeled_examples labels.\n",
        "mislabeled_indices = np.random.choice(\n",
        "    num_examples, num_mislabeled_examples, replace=False)\n",
        "labels[mislabeled_indices] = 1 - labels[mislabeled_indices]\n",
        "\n",
        "# Constant Tensors containing the labels and features.\n",
        "constant_labels = tf.constant(labels, dtype=tf.float32)\n",
        "constant_features = tf.constant(features, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5TJcpwqpjOLt"
      },
      "source": [
        "We're almost ready to construct and train our model, but first we'll create a couple of functions to measure performance. We're interested in two quantities: the average hinge loss (which we seek to minimize), and the recall (which we constrain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tGikzQ0fjSg7"
      },
      "outputs": [],
      "source": [
        "def average_hinge_loss(labels, predictions):\n",
        "  # Recall that the labels are binary (0 or 1).\n",
        "  signed_labels = (labels * 2) - 1\n",
        "  return np.mean(np.maximum(0.0, 1.0 - signed_labels * predictions))\n",
        "\n",
        "def recall(labels, predictions):\n",
        "  # Recall that the labels are binary (0 or 1).\n",
        "  positive_count = np.sum(labels)\n",
        "  true_positives = labels * (predictions \u003e 0)\n",
        "  true_positive_count = np.sum(true_positives)\n",
        "  return true_positive_count / positive_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-h9cRzOjkFH0"
      },
      "source": [
        "## Constructing and Optimizing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJL8aqDomWgT"
      },
      "source": [
        "The first step is to create the [KerasPlaceholder](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/keras.py)s that we'll need. Even in eager mode, these objects act similarly to graph-mode placeholders, in that they initially contain no values, but will be filled-in later (when the Keras loss function is called).\n",
        "\n",
        "They're parameterized by a function that takes the same parameters as a Keras loss function (prediction and labels), and returns the Tensor that the placeholder should represent. In this case, tfco_predictions returns the predictions themselves, and tfco_labels returns the labels themselves, but in more complex settings, one might need to extract multiple different quantities (e.g. protected class information, the predictions of a baseline model, etc.) from the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Y5h0YZGmh-0G"
      },
      "outputs": [],
      "source": [
        "tfco_predictions = tfco.KerasPlaceholder(lambda _, y_pred: y_pred)\n",
        "tfco_labels = tfco.KerasPlaceholder(lambda y_true, _: y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q48ZK621sgNk"
      },
      "source": [
        "The main motivation of TFCO is to make it easy to create and optimize constrained problems written in terms of linear combinations of *rates*, where a \"rate\" is the proportion of training examples on which an event occurs (e.g. the false positive rate, which is the number of negatively-labeled examples on which the model makes a positive prediction, divided by the number of negatively-labeled examples). Our current example (minimizing a hinge relaxation of the error rate subject to a recall constraint) is such a problem.\n",
        "\n",
        "Using the placeholders defined above, we are now able to define the problem to optimize. The [KerasLayer](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/keras.py) interface is similar to the [RateMinimizationProblem](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/rate_minimization_problem.py) interface, in that its two main parameters are the expression to minimize, and a list of constraints. Unlike a [RateMinimizationProblem](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/rate_minimization_problem.py), however, it also requires a list of all placeholders that are required by its inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ebAR3ER7sdOS"
      },
      "outputs": [],
      "source": [
        "context = tfco.rate_context(predictions=tfco_predictions, labels=tfco_labels)\n",
        "tfco_layer = tfco.KerasLayer(\n",
        "    tfco.error_rate(context), [tfco.recall(context) \u003e= recall_lower_bound],\n",
        "    placeholders=[tfco_predictions, tfco_labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SpXDN8KWtkGK"
      },
      "source": [
        "A [KerasLayer](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/tensorflow_constrained_optimization/python/rates/keras.py) plays two roles.\n",
        "\n",
        "\n",
        "1.   It defines the optimization problem, in terms of an objective and constraints. To this end, it also contains the loss function that should be passed to Keras' Model.compile() method.\n",
        "2.   It also contains the internal state needed by TFCO. For this reason, it must be included somewhere in the Keras model. It doesn't matter *where* it's included, since from the perspective of the model, it's an identity function. However, it must be included *somewhere*, so that the internal TFCO state will be updated during optimization.\n",
        "\n",
        "We now construct our model. As in [README.md](https://github.com/google-research/tensorflow_constrained_optimization/tree/master/README.md), we're using a linear model with a bias. Notice that we include tfco_layer in the Sequential model, which ensures that the TFCO internal state will be updated during optimization. We also pass tfco_layer.loss to the Model.compile() function, which causes us to optimize the correct constrained objective. The placeholders that we constructed earlier will be filled-in when tfco_layer.loss() is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 36958,
          "status": "ok",
          "timestamp": 1597790506569,
          "user": {
            "displayName": "Andrew Cotter",
            "photoUrl": "",
            "userId": "01600804956648002768"
          },
          "user_tz": 420
        },
        "id": "MEUrkxnmPFud",
        "outputId": "3ced817b-c0f4-4507-c3ea-03e0440760be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constrained average hinge loss = 0.745828\n",
            "Constrained recall = 0.902299\n"
          ]
        }
      ],
      "source": [
        "# You can put the tfco.KerasLayer anywhere in the sequence--its only purpose is\n",
        "# to contain the slack variables, denominators, Lagrange multipliers, and loss.\n",
        "# It's a NO-OP (more accurately, an identity function) as far as the model is\n",
        "# concerned.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=None, input_shape=(dimension,)),\n",
        "    tfco_layer\n",
        "])\n",
        "\n",
        "# Notice that we take the loss function from the tfco.KerasLayer, instead of\n",
        "# using tf.keras.losses.Hinge(), as we did above.\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=1.0),\n",
        "    loss=tfco_layer.loss)\n",
        "model.fit(constant_features, constant_labels, epochs=1000, verbose=0)\n",
        "\n",
        "trained_predictions = np.ndarray.flatten(model.predict(features))\n",
        "print(\"Constrained average hinge loss = %f\" %\n",
        "      average_hinge_loss(labels, trained_predictions))\n",
        "print(\"Constrained recall = %f\" % recall(labels, trained_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aG1eeGIvkl7D"
      },
      "source": [
        "As we hoped, the recall is extremely close to 90%\u0026mdash;and, thanks to the fact that the optimizer uses a (hinge) proxy constraint only when needed, and the actual (zero-one) constraint whenever possible, this is the *true* recall, not a hinge approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-SU9jymGjqf7"
      },
      "source": [
        "### Unconstrained Model\n",
        "\n",
        "For comparison, let's try optimizing the same problem *without* the recall constraint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 58243,
          "status": "ok",
          "timestamp": 1597790527866,
          "user": {
            "displayName": "Andrew Cotter",
            "photoUrl": "",
            "userId": "01600804956648002768"
          },
          "user_tz": 420
        },
        "id": "89xgIF7fadOo",
        "outputId": "49800743-b209-4657-a8b6-8365188ac606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unconstrained average hinge loss = 0.630985\n",
            "Unconstrained recall = 0.781609\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=None, input_shape=(dimension,))\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adagrad(learning_rate=1.0),\n",
        "    loss=tf.keras.losses.Hinge())\n",
        "model.fit(constant_features, constant_labels, epochs=1000, verbose=0)\n",
        "\n",
        "trained_predictions = np.ndarray.flatten(model.predict(features))\n",
        "print(\"Unconstrained average hinge loss = %f\" % average_hinge_loss(\n",
        "    labels, trained_predictions))\n",
        "print(\"Unconstrained recall = %f\" % recall(labels, trained_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MxL4xf9bkvkj"
      },
      "source": [
        "Because there is no constraint, the unconstrained problem does a better job of minimizing the average hinge loss, but naturally doesn't approach 90% recall."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Recall_constraint_keras.ipynb",
      "provenance": [
        {
          "file_id": "1KYQ6SqWmq5NcTEMCg9hjT5FshMSdO4Dn",
          "timestamp": 1597790785857
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
