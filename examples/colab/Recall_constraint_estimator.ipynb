{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recall_constraint_estimator.ipynb",
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj5Lwtb1LKlm"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Constrained Optimization Authors. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "> http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_R0jiDvLPwX"
      },
      "source": [
        "## Problem Setup\n",
        "\n",
        "In this colab, we'll show how to use the TF Constrained Optimization (TFCO) library with canned TF estimators. We demonstrate this on a simple recall-constrained optimization problem on simulated data: we seek a classifier that minimizes the average hinge loss while constraining recall to be at least 90%.\n",
        "\n",
        "We'll start with the required imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQi5HqA7LRhN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566651756,
          "user_tz": 420,
          "elapsed": 147,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow.compat.v2 as tf"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKgG_O3OpuqM"
      },
      "source": [
        "# Tensorflow constrained optimization library\n",
        "!pip install git+https://github.com/google-research/tensorflow_constrained_optimization\n",
        "import tensorflow_constrained_optimization as tfco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npiEaDa8LT3-"
      },
      "source": [
        "We'll next create a simple simulated dataset by sampling 1000 random 10-dimensional feature vectors from a Gaussian, finding their labels using a random \"ground truth\" linear model, and then adding noise by randomly flipping 200 labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZiNAGelLVgB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566197642,
          "user_tz": 420,
          "elapsed": 162,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "# Create a simulated 10-dimensional training dataset consisting of 1000 labeled\n",
        "# examples, of which 800 are labeled correctly and 200 are mislabeled.\n",
        "num_examples = 1000\n",
        "num_mislabeled_examples = 200\n",
        "dimension = 10\n",
        "# We will constrain the recall to be at least 90%.\n",
        "recall_lower_bound = 0.9\n",
        "\n",
        "# Create random \"ground truth\" parameters for a linear model.\n",
        "ground_truth_weights = np.random.normal(size=dimension) / math.sqrt(dimension)\n",
        "ground_truth_threshold = 0\n",
        "\n",
        "# Generate a random set of features for each example.\n",
        "features = np.random.normal(size=(num_examples, dimension)).astype(\n",
        "    np.float32) / math.sqrt(dimension)\n",
        "# Compute the labels from these features given the ground truth linear model.\n",
        "labels = (np.matmul(features, ground_truth_weights) >\n",
        "          ground_truth_threshold).astype(np.float32)\n",
        "# Add noise by randomly flipping num_mislabeled_examples labels.\n",
        "mislabeled_indices = np.random.choice(\n",
        "    num_examples, num_mislabeled_examples, replace=False)\n",
        "labels[mislabeled_indices] = 1 - labels[mislabeled_indices]\n",
        "\n",
        "# Constant Tensors containing the labels and features.\n",
        "constant_labels = tf.constant(labels, dtype=tf.float32)\n",
        "constant_features = tf.constant(features, dtype=tf.float32)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKPWLso3-ww"
      },
      "source": [
        "## Training with a Canned Estimator\n",
        "\n",
        "We now show how to train a canned `LinearEstimator` using the custom head that TFCO provides. For this, we'll need to create function that takes \"logits\", \"labels\", \"features\", and an (optional) \"weight_column\", and returns a\n",
        "`RateMinimizationProblem`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Wkdf6mI5tG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566198395,
          "user_tz": 420,
          "elapsed": 2,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "def problem_fn(logits, labels, features, weight_column=None):\n",
        "  # Minimize error rate s.t. recall >= recall_lower_bound.\n",
        "  del features, weight_column\n",
        "  context = tfco.rate_context(logits, labels)\n",
        "  objective = tfco.error_rate(context)\n",
        "  constraints = [tfco.recall(context) >= recall_lower_bound]\n",
        "  return tfco.RateMinimizationProblem(objective, constraints)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pdT8qpPrFr0"
      },
      "source": [
        "Next, we create a  custom `tfco.Head` that wraps around an existing binary classification head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFVUhccHrJaW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566199303,
          "user_tz": 420,
          "elapsed": 3,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "binary_head = tf.estimator.BinaryClassHead()\n",
        "head = tfco.HeadV2(binary_head, problem_fn)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uppYKix2-SdA"
      },
      "source": [
        "All that remains is to set up the input pipeline. We first create `feature_columns` to convert the dataset into a format that can be processed by an estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_klY8Dqueag_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566200227,
          "user_tz": 420,
          "elapsed": 54,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "feature_columns = []\n",
        "for ii in range(features.shape[-1]):\n",
        "  feature_columns.append(\n",
        "      tf.feature_column.numeric_column(str(ii), dtype=tf.float32))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyaRvuOXAGt-"
      },
      "source": [
        "We next construct the input functions that return the data to be used by the estimator for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoEGUpg9pTdD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566201094,
          "user_tz": 420,
          "elapsed": 57,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        }
      },
      "source": [
        "def make_input_fn(\n",
        "    data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_fn():\n",
        "    features_dict = {\n",
        "        str(ii): features[:, ii] for ii in range(features.shape[-1])}\n",
        "    ds = tf.data.Dataset.from_tensor_slices((features_dict, labels))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    return ds\n",
        "  return input_fn\n",
        "\n",
        "train_input_fn = make_input_fn(features, labels, num_epochs=1000)\n",
        "test_input_fn = make_input_fn(features, labels, num_epochs=1, shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSD3NMDpJMk-"
      },
      "source": [
        "We are now ready to train the estimator. We'll pass the `ProxyLagrangianOptimizer` that TFCO provides to the estimator. We could also instead use a standard TF optimizer here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TImVz7WMp-Nb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566209950,
          "user_tz": 420,
          "elapsed": 8036,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        },
        "outputId": "2e391c16-f995-402c-d21b-ed00ceb6b5bb"
      },
      "source": [
        "# Create a temporary model directory.\n",
        "model_dir = \"tfco_tmp\"\n",
        "if os.path.exists(model_dir):\n",
        "  shutil.rmtree(model_dir)\n",
        "\n",
        "# Train estimator with TFCO's custom optimizer.\n",
        "optimizer = tfco.ProxyLagrangianOptimizer(\n",
        "    tf.keras.optimizers.Adagrad(1))\n",
        "estimator = tf.estimator.LinearEstimator(\n",
        "    head, feature_columns, model_dir=model_dir, \n",
        "    optimizer=optimizer)\n",
        "estimator.train(train_input_fn, steps=1000) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.linear.LinearEstimatorV2 at 0x7fbbb02682b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTOFzeWz-Ga4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1624566214450,
          "user_tz": 420,
          "elapsed": 1718,
          "user": {
            "displayName": "Harikrishna Narasimhan",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhPpxfCFEpxXPF8TjslNEdEjWPHaUygxJI3RTWSSg=s64",
            "userId": "06672706079361346307"
          }
        },
        "outputId": "a9006017-f5ce-4413-e08e-e6e30ca651ef"
      },
      "source": [
        "estimator.evaluate(test_input_fn)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.664,\n",
              " 'accuracy_baseline': 0.503,\n",
              " 'auc': 0.8110252,\n",
              " 'auc_precision_recall': 0.7720304,\n",
              " 'average_loss': 0.6005209,\n",
              " 'label/mean': 0.503,\n",
              " 'loss': 0.59834015,\n",
              " 'precision': 0.6129905,\n",
              " 'prediction/mean': 0.635723,\n",
              " 'recall': 0.90059644,\n",
              " 'global_step': 1000}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ZBLtO_sVWm"
      },
      "source": [
        "Notice that the recall is close to 90% as desired."
      ]
    }
  ]
}
